<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>EchoFocus | AI Automated Echocardiography</title>
<meta name="generator" content="Jekyll v3.10.0" />
<meta property="og:title" content="EchoFocus" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="AI Automated Echocardiography" />
<meta property="og:description" content="AI Automated Echocardiography" />
<meta property="og:site_name" content="EchoFocus" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="EchoFocus" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebSite","description":"AI Automated Echocardiography","headline":"EchoFocus","name":"EchoFocus","publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"/assets/images/logo.png"}},"url":"/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=75732202fa5d12267f43a1e3951b91dd63f42769">
    <!--[if lt IE 9]>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1><a href="/">EchoFocus</a></h1>

        
          <img src="/assets/images/logo.png" alt="Logo" />
        

        <p>AI Automated Echocardiography</p>

        
        <p class="view"><a href="https://github.com/cavalab/echofocus">View the Project on GitHub <small>cavalab/echofocus</small></a></p>
        

        

        
      </header>
      <section>

      <h1 id="echofocus">EchoFocus</h1>

<p>EchoFocus is an AI method for echocardiography that diagnoses and measures cardiac function based on videos comprising an echocardiogram study. <br />
Its name refers to the fact that EchoFocus skips view classification, instead relying on attention mechanisms to determine which echo views to priortize in making specfic predictions.</p>

<h1 id="install">Install</h1>

<p>Project dependencies are in <code class="language-plaintext highlighter-rouge">pyproject.toml</code> and <code class="language-plaintext highlighter-rouge">uv.lock</code>.</p>

<h2 id="requirements">Requirements:</h2>

<ul>
  <li>Python &gt;= 3.9</li>
  <li>Cuda-enabled NVIDA GPU</li>
</ul>

<h2 id="quickstart-uv">Quickstart (uv)</h2>

<p>Using <a href="https://docs.astral.sh/uv/">uv</a>:</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>uv <span class="nb">sync</span> <span class="c"># sync dependencies</span>
uv run echofocus.py <span class="nt">--help</span>
</code></pre></div></div>

<h2 id="quickstart-pip">Quickstart (pip)</h2>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
python echofocus.py --help
</code></pre></div></div>

<h1 id="usage">Usage</h1>

<h1 id="extract-video-embeddings">Extract video embeddings</h1>

<p>EchoFocus works on top of the video embeddings generated by <a href="https://github.com/CarDS-Yale/PanEcho/">PanEcho</a>. 
In order to use EchoFocus, you must first generate video embeddings for your echos, and store them in <code class="language-plaintext highlighter-rouge">hd5</code> files. 
See the <a href="https://github.com/cavalab/echofocus/tree/main/embed">embed</a> for scripts and guidelines on how to accomplish this.</p>

<p>From there, you can use EchoFocus to train models, generate study embeddings, generate new predictions, and analyze video importance according to the functions below.</p>

<h2 id="load-a-pre-trained-model">Load a pre-trained model</h2>

<p>Pre-trained models are made available via the <a href="https://github.com/cavalab/echofocus/releases">Releases</a> page. 
Once downloaded, they can be used through calls to <code class="language-plaintext highlighter-rouge">echofocus.py</code> as described below.</p>

<h2 id="train-models">Train Models</h2>

<p>To train a model, you must first create a configuration file named <code class="language-plaintext highlighter-rouge">config.json</code>. 
An example file called <code class="language-plaintext highlighter-rouge">config-example.json</code> is included as a template for you to use. 
The <code class="language-plaintext highlighter-rouge">config.json</code> file specifies:</p>
<ul>
  <li>for each task configuration, and the path to the labels file, and the columns in the labels file to use for training.</li>
  <li>for each dataset, the the path to the pre-generated PanEcho video embeddings.</li>
</ul>

<p>Once <code class="language-plaintext highlighter-rouge">config.json</code> is made, you can train models by calling <code class="language-plaintext highlighter-rouge">python echofocus.py train</code> and specifying a model name, a dataset, and task.</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python echofocus.py train \
    --model_name [model_name] \
    --dataset [dataset] \
    --task [measure,chd,fyler]
</code></pre></div></div>

<ul>
  <li>Trained models and training results are stored in <code class="language-plaintext highlighter-rouge">./trained_models/[model_name]</code>. If the <code class="language-plaintext highlighter-rouge">model_name</code> you specify already exists, that one will be loaded from the best checkpoint and training will resume according to the specified arguments.</li>
  <li>Task information is stored in <code class="language-plaintext highlighter-rouge">config.json</code>.</li>
  <li>The <code class="language-plaintext highlighter-rouge">dataset</code> is used as a key lookup to <code class="language-plaintext highlighter-rouge">config['dataset'][dataset]</code> to load the paths to the label files.</li>
</ul>

<h2 id="generate-study-embeddings">Generate Study Embeddings</h2>

<p>To generate study level embeddings, use <code class="language-plaintext highlighter-rouge">python echofocus.py embed</code>. 
For example:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python echofocus.py embed \
    --dataset outside \
    --model_name EchoFocus_Measure
</code></pre></div></div>

<p>Would generate study embeddings using the EchoFocus_Measure model on the “outside” dataset.</p>

<h2 id="explain-model-outputs">Explain Model Outputs</h2>

<p>EchoFocus supports model explanations that attribute importance to individual videos in the echo study using integrated gradients. 
This is accomplished by calling <code class="language-plaintext highlighter-rouge">python echofocus.py explain</code> with the appropriate arguments.</p>

<h1 id="cite">Cite</h1>

<p>Platon Lukyanenko, Sunil Ghelani, Yuting Yang, Bohan Jiang, Timothy Miller, David Harrild, Nao Sasaki, Francesca Sperotto, Danielle Sganga, John Triedman, Andrew J. Powell, Tal Geva, William G. La Cava, Joshua Mayourian (2026).
<strong>Automated Echocardiographic Detection of Congenital Heart Disease Using Artificial Intelligence</strong>.
Preprint: <a href="https://www.medrxiv.org/content/10.64898/2026.01.24.26344771v1">medrxiv.org</a></p>

<h1 id="contact">Contact</h1>

<p>This work is a joint project of the <a href="https://research.childrenshospital.org/research-units/congenital-heart-artificial-intelligence-lab">Congenital Heart AI Lab (CHAI Lab)</a> and the <a href="https://cavalab.org">Cava Lab</a> at Boston Children’s Hospital, affiliated with Harvard Medical School.</p>

<p>To get help with the repository, <a href="https://github.com/cavalab/echofocus/issues">create an issue</a>. 
PR contributions are very welcome.</p>

<h2 id="maintainers">Maintainers</h2>

<ul>
  <li>William G. La Cava (<a href="https://github.com/lacava">@lacava</a>)</li>
  <li>Platon Lukyanenko</li>
  <li>Joshua Mayourian</li>
</ul>

<h1 id="acknowledgments">Acknowledgments</h1>

<p>The authors would like to acknowledge Boston Children’s Hospital’s High-Performance Computing Resources Clusters Enkefalos 3 (E3) made available for conducting the research reported in this publication.</p>

<p>This work was supported in part by the Kostin Innovation Fund, Thrasher Research Fund Early Career Award, NIH/NHLBI T32HL007572, and NIH/NLHBI 2U01HL098147-12.</p>



      </section>
      <footer>
        
        <p>This project is maintained by <a href="https://github.com/cavalab">cavalab</a></p>
        
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
  </body>
</html>
